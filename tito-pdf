#!/usr/bin/env python3
"""tito-pdf

Local, deterministic PDF/DOCX -> Markdown converter.

Supported inputs:
- .pdf
- .docx

Outputs:
- Primary Markdown: --md-out PATH (recommended) or convenience mode writes <stem>.md
  next to the input file (or under --out-dir).
- Tables Markdown (optional): --tables-out PATH or convenience mode writes <stem>.tables.md
- Plaintext (optional): --raw-text-out PATH
- Assets/metrics JSON (optional): --assets-json PATH

Notes:
- PDF preparation requires `qpdf`.
- OCR is performed via `ocrmypdf` when enabled (requires `tesseract`).
- Headings are best-effort and depend on PDF layout/text layer quality.
"""

from __future__ import annotations

import argparse
import json
import os
import platform
import re
import shutil
import subprocess
import sys
import tempfile
import time
from collections import Counter
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

__version__ = "0.1.0"
__repo_url__ = "https://github.com/averriK/tito-PDF"
__bugs_url__ = "https://github.com/averriK/tito-PDF/issues"


def _eprint(*args: object) -> None:
    print(*args, file=sys.stderr)


def _which(cmd: str) -> Optional[str]:
    return shutil.which(cmd)


def _utc_now_iso() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


def _cmd_version(cmd: Sequence[str]) -> Optional[str]:
    if not cmd:
        return None
    exe = str(cmd[0])
    if _which(exe) is None:
        return None
    try:
        p = subprocess.run(
            list(cmd),
            text=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=5,
        )
    except Exception:
        return None

    if p.returncode != 0:
        return None
    out = (p.stdout or p.stderr or "").strip()
    if not out:
        return None
    return out.splitlines()[0].strip()


def _py_pkg_version(dist_name: str) -> Optional[str]:
    try:
        from importlib.metadata import version as _version  # py>=3.8

        return str(_version(dist_name))
    except Exception:
        return None


def _run(cmd: Sequence[str], *, cwd: Optional[Path] = None) -> None:
    # Keep logs concise (no huge stdout dumps).
    _eprint("+", " ".join(cmd))
    p = subprocess.run(
        list(cmd),
        cwd=str(cwd) if cwd else None,
        text=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    if p.returncode != 0:
        tail = (p.stderr or p.stdout or "").strip().splitlines()[-5:]
        msg = "\n".join(tail).strip()
        raise RuntimeError(f"command failed: {' '.join(cmd)}\n{msg}")


def _slugify_id(name: str) -> str:
    s = (name or "").strip()
    s = re.sub(r"\.[Pp][Dd][Ff]$", "", s)
    s = re.sub(r"\.[Dd][Oo][Cc][Xx]$", "", s)
    s = re.sub(r"[^A-Za-z0-9._-]+", "_", s)
    s = s.strip("._-")
    return s or "doc"


def _ts_run_id() -> str:
    return datetime.now().strftime("run-%Y%m%d_%H%M%S")


def _readable_bytes(n: int) -> str:
    if n < 1024:
        return f"{n}B"
    if n < 1024 * 1024:
        return f"{n/1024:.1f}KB"
    return f"{n/(1024*1024):.1f}MB"


def prepare_pdf(*, input_pdf: Path, output_pdf: Path) -> None:
    """Prepare a working copy for downstream parsing.

    This stage requires `qpdf` and always runs:
      qpdf --decrypt input.pdf output.pdf
    """

    qpdf = _which("qpdf")
    if not qpdf:
        raise RuntimeError(
            "qpdf is required for PDF conversion but was not found on PATH. "
            "Install qpdf and re-run."
        )

    output_pdf.parent.mkdir(parents=True, exist_ok=True)
    _run([qpdf, "--decrypt", str(input_pdf), str(output_pdf)])


def ocr_pdf(*, input_pdf: Path, output_pdf: Path, force: bool) -> bool:
    """Run OCR via ocrmypdf (if available). Returns True if OCR was run."""

    ocrmypdf_exe = _which("ocrmypdf")
    output_pdf.parent.mkdir(parents=True, exist_ok=True)

    use_module = False
    if not ocrmypdf_exe:
        # When running under a venv without activation, the console entrypoint may
        # not be on PATH. If the package is installed, invoke it as a module.
        if _py_pkg_version("ocrmypdf") is not None:
            ocrmypdf_exe = sys.executable
            use_module = True
        else:
            shutil.copy2(input_pdf, output_pdf)
            return False

    cmd = [ocrmypdf_exe]
    if use_module:
        cmd += ["-m", "ocrmypdf"]
    cmd += ["--quiet"]
    # Avoid rewriting text-based PDFs unless forced.
    cmd += ["--force-ocr"] if force else ["--skip-text"]
    # Keep output deterministic-ish and stable.
    cmd += ["--output-type", "pdf"]
    cmd += [str(input_pdf), str(output_pdf)]

    try:
        _run(cmd)
        return True
    except Exception as e:
        _eprint(f"WARNING: OCR failed ({e}); continuing without OCR")
        shutil.copy2(input_pdf, output_pdf)
        return False


@dataclass(frozen=True)
class PdfLine:
    page: int
    text: str
    size: float
    x0: float
    y0: float
    x1: float
    y1: float
    page_w: float
    page_h: float
    bold: bool


def _norm_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()


def _looks_like_page_number(s: str) -> bool:
    t = _norm_text(s).lower()
    if re.fullmatch(r"\d{1,4}", t or ""):
        return True
    if re.fullmatch(r"page\s+\d{1,4}(\s+of\s+\d{1,4})?", t or ""):
        return True
    return False


def extract_lines_layout(pdf_path: Path, *, max_pages: int = 0) -> List[PdfLine]:
    try:
        import fitz  # PyMuPDF
    except Exception as e:
        raise SystemExit(
            "PyMuPDF is required for layout-aware extraction. Install it with: pip install PyMuPDF\n"
            f"Import error: {e}"
        )

    doc = fitz.open(str(pdf_path))
    out: List[PdfLine] = []

    n_pages = doc.page_count
    if max_pages and max_pages > 0:
        n_pages = min(n_pages, max_pages)

    for pno in range(n_pages):
        page = doc.load_page(pno)
        page_w = float(page.rect.width)
        page_h = float(page.rect.height)
        d = page.get_text("dict")
        for b in d.get("blocks", []):
            if b.get("type") != 0:
                continue
            for ln in b.get("lines", []):
                spans = ln.get("spans", [])
                parts: List[str] = []
                sizes: List[float] = []
                fonts: List[str] = []
                for sp in spans:
                    t = sp.get("text", "")
                    if not t:
                        continue
                    parts.append(t)
                    try:
                        sizes.append(float(sp.get("size", 0.0)))
                    except Exception:
                        pass
                    fonts.append(str(sp.get("font", "")))

                txt = _norm_text("".join(parts))
                if not txt:
                    continue

                x0, y0, x1, y1 = map(float, ln.get("bbox", [0, 0, 0, 0]))
                size = 0.0
                if sizes:
                    sizes2 = sorted(sizes)
                    size = float(sizes2[len(sizes2) // 2])

                bold = any("bold" in f.lower() for f in fonts)

                out.append(
                    PdfLine(
                        page=pno + 1,
                        text=txt,
                        size=size,
                        x0=x0,
                        y0=y0,
                        x1=x1,
                        y1=y1,
                        page_w=page_w,
                        page_h=page_h,
                        bold=bold,
                    )
                )

    return out


def drop_repeated_headers_footers(lines: List[PdfLine]) -> List[PdfLine]:
    # If too few pages, this heuristic is noisy.
    pages = sorted({l.page for l in lines})
    if len(pages) < 3:
        return [l for l in lines if not _looks_like_page_number(l.text)]

    top = Counter()
    bottom = Counter()
    per_page: Dict[int, List[PdfLine]] = {}
    for l in lines:
        per_page.setdefault(l.page, []).append(l)

    for p in pages:
        page_lines = per_page.get(p, [])
        for l in page_lines:
            t = _norm_text(l.text)
            if not t:
                continue
            if len(t) > 80:
                continue
            if l.y1 <= 0.12 * l.page_h:
                top[t] += 1
            if l.y0 >= 0.88 * l.page_h:
                bottom[t] += 1

    thresh = max(2, int(len(pages) * 0.6))
    drop = {t for (t, c) in top.items() if c >= thresh}
    drop |= {t for (t, c) in bottom.items() if c >= thresh}

    out: List[PdfLine] = []
    for l in lines:
        t = _norm_text(l.text)
        if _looks_like_page_number(t):
            continue
        if t in drop:
            continue
        out.append(l)

    return out


def infer_body_font_size(lines: List[PdfLine]) -> float:
    # Use mode of rounded sizes on "normal" length lines.
    sizes: List[float] = []
    for l in lines:
        if l.size <= 0 or l.size > 72:
            continue
        t = l.text.strip()
        if len(t) < 30:
            continue
        sizes.append(round(l.size * 2) / 2.0)  # round to 0.5

    if not sizes:
        return 12.0

    c = Counter(sizes)
    body = c.most_common(1)[0][0]
    return float(body)


def _is_centered(l: PdfLine) -> bool:
    cx = (l.x0 + l.x1) / 2.0
    return abs(cx - (l.page_w / 2.0)) <= (l.page_w * 0.12)


def _is_heading(l: PdfLine, body_size: float) -> bool:
    t = l.text.strip()
    if len(t) < 2 or len(t) > 140:
        return False

    # Avoid treating sentences as headings.
    if t.endswith(".") and len(t) > 40:
        return False

    # Primary: size-based.
    if l.size >= body_size * 1.35:
        return True

    # Fallbacks for OCR/uniform-font PDFs.
    if l.bold and len(t) <= 80:
        return True
    if _is_centered(l) and len(t) <= 80 and (t.isupper() or re.match(r"^\d+(?:\.\d+)*\s+\S+", t)):
        return True
    if t.isupper() and 5 <= len(t) <= 60:
        return True

    return False


def _heading_level(l: PdfLine, body_size: float) -> int:
    # 1..6
    if l.size >= body_size * 1.7:
        return 1
    if l.size >= body_size * 1.5:
        return 2
    if l.size >= body_size * 1.35:
        return 3

    # OCR/uniform font fallbacks
    t = l.text.strip()
    m = re.match(r"^(\d+)(?:\.(\d+))?(?:\.(\d+))?\s+", t)
    if m:
        # More dots -> deeper level
        dots = sum(1 for g in m.groups()[1:] if g)
        return min(4 + dots, 6)

    return 3


def _is_list_item(text: str) -> bool:
    t = text.lstrip()
    if re.match(r"^[-*•]\s+\S+", t):
        return True
    if re.match(r"^\d+[\).]\s+\S+", t):
        return True
    return False


def _clean_list_marker(text: str) -> str:
    t = text.lstrip()
    if re.match(r"^\d+[\).]\s+", t):
        return re.sub(r"^\d+[\).]\s+", "1. ", t)
    if re.match(r"^[-*•]\s+", t):
        return re.sub(r"^[-*•]\s+", "- ", t)
    return "- " + t


def lines_to_markdown(lines: List[PdfLine]) -> str:
    # Sort in reading order: page, y0, x0
    lines2 = sorted(lines, key=lambda l: (l.page, l.y0, l.x0))
    body = infer_body_font_size(lines2)

    out: List[str] = []
    para = ""
    prev: Optional[PdfLine] = None

    def flush_para() -> None:
        nonlocal para
        if para.strip():
            out.append(para.strip())
            out.append("")
        para = ""

    for l in lines2:
        t = l.text.strip()
        if not t:
            continue

        if _is_heading(l, body):
            flush_para()
            level = _heading_level(l, body)
            out.append("#" * level + " " + t)
            out.append("")
            prev = l
            continue

        if _is_list_item(t):
            flush_para()
            out.append(_clean_list_marker(t))
            prev = l
            continue

        # Paragraph joining
        if not para:
            para = t
            prev = l
            continue

        gap = 0.0
        if prev is not None and prev.page == l.page:
            gap = max(0.0, l.y0 - prev.y1)

        # New paragraph on big vertical gap.
        if gap > max(6.0, body * 0.9):
            flush_para()
            para = t
            prev = l
            continue

        # Hyphenation repair
        if para.endswith("-") and t and t[0].islower():
            para = para[:-1] + t
        else:
            para = para + " " + t

        prev = l

    flush_para()

    # Trim trailing blank lines.
    while out and out[-1] == "":
        out.pop()

    return "\n".join(out).rstrip("\n") + "\n"


def lines_to_text(lines: List[PdfLine]) -> str:
    """Plaintext export for downstream slicing/cleaning (no Markdown)."""

    # Sort in reading order: page, y0, x0
    lines2 = sorted(lines, key=lambda l: (l.page, l.y0, l.x0))
    body = infer_body_font_size(lines2)

    out: List[str] = []
    para = ""
    prev: Optional[PdfLine] = None

    def flush_para() -> None:
        nonlocal para
        if para.strip():
            out.append(para.strip())
            out.append("")
        para = ""

    for l in lines2:
        t = l.text.strip()
        if not t:
            continue

        if not para:
            para = t
            prev = l
            continue

        gap = 0.0
        if prev is not None and prev.page == l.page:
            gap = max(0.0, l.y0 - prev.y1)

        # New paragraph on page breaks or big vertical gap.
        if prev is not None and prev.page != l.page:
            flush_para()
            para = t
            prev = l
            continue
        if gap > max(6.0, body * 0.9):
            flush_para()
            para = t
            prev = l
            continue

        # Hyphenation repair
        if para.endswith("-") and t and t[0].islower():
            para = para[:-1] + t
        else:
            para = para + " " + t

        prev = l

    flush_para()

    # Trim trailing blank lines.
    while out and out[-1] == "":
        out.pop()

    return "\n".join(out).rstrip("\n") + "\n"


def _escape_cell(s: str) -> str:
    return (s or "").replace("|", "\\|").replace("\n", " ").strip()


def rows_to_markdown_table(rows: List[List[str]]) -> Optional[List[str]]:
    if not rows:
        return None

    # Drop fully-empty rows
    rows2 = []
    for r in rows:
        rr = ["" if c is None else str(c) for c in r]
        if any(_norm_text(c) for c in rr):
            rows2.append(rr)

    if not rows2:
        return None

    max_cols = max(len(r) for r in rows2)
    if max_cols < 2:
        return None

    norm: List[List[str]] = []
    for r in rows2:
        if len(r) < max_cols:
            r = r + ([""] * (max_cols - len(r)))
        elif len(r) > max_cols:
            head = r[: max_cols - 1]
            tail = r[max_cols - 1 :]
            r = head + [" / ".join(tail)]
        norm.append([_escape_cell(c) for c in r])

    header = norm[0]
    if not any(_norm_text(c) for c in header):
        header = [f"Col{i+1}" for i in range(max_cols)]

    sep = ["---"] * max_cols

    out: List[str] = []
    out.append("| " + " | ".join(header) + " |")
    out.append("| " + " | ".join(sep) + " |")

    for r in norm[1:]:
        out.append("| " + " | ".join(r) + " |")

    return out


def extract_tables(
    pdf_path: Path,
    *,
    max_pages: int = 0,
    lenient: bool = False,
) -> Tuple[List[Dict[str, Any]], List[str]]:
    """Return (audit_tables, tables_markdown_lines).

    Goal: robust *deterministic* table extraction (no LLM).

    Strategy:
    - Prefer PyMuPDF's built-in table finder first (already a dependency for text extraction).
    - Optional lenient mode enables text-based strategies (higher recall, higher risk).
    - Deduplicate tables across strategies by content signature.
    """

    audit: List[Dict[str, Any]] = []
    out_md: List[str] = []
    seen: set[str] = set()

    def table_signature(rows: List[List[str]]) -> str:
        import hashlib

        parts: List[str] = []
        for r in rows:
            parts.append("|".join(_norm_text(str(c or "")) for c in r))
        blob = "\n".join(parts).encode("utf-8", errors="replace")
        return hashlib.sha1(blob).hexdigest()

    def table_stats(rows: List[List[str]]) -> Dict[str, Any]:
        cells = [c for r in rows for c in r]
        nonempty = [c for c in cells if _norm_text(c)]

        empty_ratio = 1.0
        if cells:
            empty_ratio = 1.0 - (len(nonempty) / max(1, len(cells)))

        digit_cells = [c for c in nonempty if re.search(r"\d", c)]
        digit_ratio = (len(digit_cells) / max(1, len(nonempty))) if nonempty else 0.0

        return {
            "rows": len(rows),
            "cols": max((len(r) for r in rows), default=0),
            "cells_total": len(cells),
            "cells_nonempty": len(nonempty),
            "empty_ratio": round(float(empty_ratio), 4),
            "digit_ratio": round(float(digit_ratio), 4),
        }

    def bbox_stats(*, bbox: Tuple[float, float, float, float], page_w: float, page_h: float) -> Dict[str, Any]:
        x0, top, x1, bottom = bbox
        w = max(0.0, float(x1) - float(x0))
        h = max(0.0, float(bottom) - float(top))
        area = w * h
        page_area = max(1.0, float(page_w) * float(page_h))
        return {
            "bbox": [float(x0), float(top), float(x1), float(bottom)],
            "width_ratio": round(w / max(1.0, float(page_w)), 4),
            "height_ratio": round(h / max(1.0, float(page_h)), 4),
            "area_ratio": round(area / page_area, 4),
            "top_ratio": round(float(top) / max(1.0, float(page_h)), 4),
            "bottom_ratio": round(float(bottom) / max(1.0, float(page_h)), 4),
        }

    def should_accept(
        *,
        stats: Dict[str, Any],
        bbox_meta: Optional[Dict[str, Any]],
        mode: str,
        tool: str,
    ) -> Tuple[bool, str]:
        r = int(stats.get("rows", 0) or 0)
        c = int(stats.get("cols", 0) or 0)
        empty_ratio = float(stats.get("empty_ratio", 1.0) or 1.0)
        digit_ratio = float(stats.get("digit_ratio", 0.0) or 0.0)

        cells_total = int(stats.get("cells_total", 0) or 0)
        cells_nonempty = int(stats.get("cells_nonempty", 0) or 0)

        if r < 2 or c < 2:
            return False, "too_small"
        if c > 30:
            return False, "too_many_cols"
        if r > 500:
            return False, "too_many_rows"

        # Drop very small 2-row grids unless they are almost fully populated.
        # (Common false-positive: drawing title blocks and page furniture.)
        if r == 2 and c >= 3:
            filled = (cells_nonempty / max(1, cells_total)) if cells_total else 0.0
            if filled < 0.90:
                return False, "two_row_sparse_grid"

        if empty_ratio > 0.85:
            return False, "too_sparse"

        # Drop tiny header/footer blocks (often page furniture).
        if bbox_meta:
            hr = float(bbox_meta.get("height_ratio", 0.0) or 0.0)
            tr = float(bbox_meta.get("top_ratio", 0.0) or 0.0)
            br = float(bbox_meta.get("bottom_ratio", 0.0) or 0.0)
            ar = float(bbox_meta.get("area_ratio", 0.0) or 0.0)

            if hr < 0.05 and r <= 6 and (tr < 0.12 or br > 0.88) and digit_ratio < 0.35:
                return False, "tiny_header_footer"

            # Drop tiny sparse blocks anywhere on the page (common false-positive: title blocks / page furniture).
            if hr < 0.05 and ar < 0.05 and empty_ratio > 0.55 and digit_ratio < 0.50:
                return False, "tiny_sparse_block"

        # PyMuPDF "lines/lines" can sometimes interpret multi-column prose as tables.
        # A strong signal for that failure mode is a consistently-narrow bbox (~one text column)
        # coupled with high sparsity and low numeric density.
        if tool == "pymupdf" and bbox_meta and mode == "pymupdf/lines/lines":
            wr = float(bbox_meta.get("width_ratio", 1.0) or 1.0)
            if wr < 0.75:
                # In multi-column PDFs, PyMuPDF often finds "tables" confined to a single text column.
                # Treat these as false positives unless they are strongly table-like.
                if not (digit_ratio >= 0.60 and empty_ratio <= 0.60 and r >= 4 and c >= 3):
                    return False, "narrow_pymupdf_column"

        # Guard against multi-column prose being mis-detected as a table.
        if "text" in (mode or ""):
            if not bbox_meta:
                return False, "missing_bbox"
            ar = float(bbox_meta.get("area_ratio", 1.0) or 1.0)
            wr = float(bbox_meta.get("width_ratio", 1.0) or 1.0)
            hr = float(bbox_meta.get("height_ratio", 1.0) or 1.0)

            # Heuristic defaults:
            # - text-strategy tables should not be near-full-page
            # - allow large tables only if they look numeric (digit_ratio)
            # - should generally be wide (exclude single-column blocks)
            if (ar > 0.60 or hr > 0.60) and digit_ratio < 0.25:
                return False, "page_like_text_table"

            # Extra guard: huge, sparse text-grids are almost always prose.
            if hr > 0.85 and empty_ratio > 0.55 and digit_ratio < 0.35:
                return False, "page_like_sparse_text_table"

            if wr < 0.75:
                return False, "narrow_text_table"

        # Universal hard stop: near-full-page table is almost always a false positive.
        if bbox_meta:
            ar = float(bbox_meta.get("area_ratio", 0.0) or 0.0)
            hr = float(bbox_meta.get("height_ratio", 0.0) or 0.0)
            if ar > 0.92 and hr > 0.85:
                return False, "near_full_page"

        return True, "ok"

    def add_table(
        *,
        tool: str,
        page: int,
        rows: List[List[str]],
        meta: Dict[str, Any],
        bbox_meta: Optional[Dict[str, Any]] = None,
    ) -> None:
        # Normalize rows to strings
        rows2 = []
        for row in rows:
            rr = []
            for c in row:
                if c is None:
                    rr.append("")
                else:
                    s = str(c)
                    rr.append("" if s.strip() == "None" else s)
            rows2.append(rr)

        stats = table_stats(rows2)
        mode = str(meta.get("mode", ""))
        ok, reason = should_accept(stats=stats, bbox_meta=bbox_meta, mode=mode, tool=tool)
        if not ok:
            return

        sig = table_signature(rows2)
        if sig in seen:
            return

        md_table = rows_to_markdown_table(rows2)
        if not md_table:
            return

        seen.add(sig)

        idx = len(audit) + 1
        out_md.append(f"## Table {idx} (page {page})")
        out_md.append("")
        out_md.extend(md_table)
        out_md.append("")

        audit.append(
            {
                "index": idx,
                "page": page,
                "tool": tool,
                "mode": mode,
                "sha1": sig,
                **stats,
                **(bbox_meta or {}),
                **meta,
            }
        )

    # ---------------------------------------------------------------------
    # 1) PyMuPDF table finder (primary)
    # ---------------------------------------------------------------------
    try:
        import fitz  # PyMuPDF

        doc = fitz.open(str(pdf_path))
        n_pages = doc.page_count
        if max_pages and max_pages > 0:
            n_pages = min(n_pages, max_pages)

        combos: List[Tuple[str, str]] = [("lines", "lines")]
        if lenient:
            combos.extend(
                [
                    ("lines", "text"),
                    ("text", "lines"),
                    ("text", "text"),
                ]
            )

        for pno in range(n_pages):
            page = doc.load_page(pno)
            page_w = float(page.rect.width)
            page_h = float(page.rect.height)

            for v, h in combos:
                mode = f"pymupdf/{v}/{h}"
                try:
                    res = page.find_tables(vertical_strategy=v, horizontal_strategy=h)
                except Exception:
                    continue

                for t in getattr(res, "tables", []) or []:
                    try:
                        rows = t.extract()
                    except Exception:
                        continue

                    bm = None
                    try:
                        # bbox is (x0, y0, x1, y1)
                        bm = bbox_stats(bbox=tuple(map(float, t.bbox)), page_w=page_w, page_h=page_h)
                    except Exception:
                        bm = None

                    add_table(
                        tool="pymupdf",
                        page=pno + 1,
                        rows=rows,
                        meta={"mode": mode},
                        bbox_meta=bm,
                    )

        if audit:
            return audit, out_md

    except Exception:
        pass

    # ---------------------------------------------------------------------
    # 2) Camelot (optional)
    # ---------------------------------------------------------------------
    try:
        import camelot  # type: ignore

        flavors = ["lattice", "stream"]
        pages_opt = "all"
        if max_pages and max_pages > 0:
            pages_opt = "1-{}".format(max_pages)

        for flavor in flavors:
            tables = camelot.read_pdf(str(pdf_path), pages=pages_opt, flavor=flavor)
            if getattr(tables, "n", 0) == 0:
                continue

            for t in tables:
                page = int(getattr(t, "page", 0) or 0)
                try:
                    df = t.df
                    rows = df.values.tolist()
                except Exception:
                    continue

                add_table(
                    tool="camelot",
                    page=page or 0,
                    rows=rows,
                    meta={"flavor": flavor, "mode": f"camelot/{flavor}"},
                )

            if audit:
                return audit, out_md

    except Exception:
        pass

    # ---------------------------------------------------------------------
    # 3) pdfplumber fallback
    # ---------------------------------------------------------------------
    try:
        import pdfplumber  # type: ignore

        with pdfplumber.open(str(pdf_path)) as pdf:
            n_pages = len(pdf.pages)
            if max_pages and max_pages > 0:
                n_pages = min(n_pages, max_pages)

            settings_list: List[Dict[str, Any]] = [
                {"vertical_strategy": "lines", "horizontal_strategy": "lines"},
            ]
            if lenient:
                settings_list.extend(
                    [
                        {"vertical_strategy": "lines", "horizontal_strategy": "text"},
                        {"vertical_strategy": "text", "horizontal_strategy": "lines"},
                        {"vertical_strategy": "text", "horizontal_strategy": "text"},
                    ]
                )

            for i in range(n_pages):
                page = pdf.pages[i]
                page_w = float(page.width)
                page_h = float(page.height)

                for settings in settings_list:
                    v = str(settings.get("vertical_strategy", ""))
                    h = str(settings.get("horizontal_strategy", ""))
                    mode = f"pdfplumber/{v}/{h}"

                    try:
                        found = page.find_tables(table_settings=settings)
                    except Exception:
                        continue

                    for t in found or []:
                        try:
                            rows = t.extract()
                        except Exception:
                            continue

                        bm = None
                        try:
                            bm = bbox_stats(bbox=tuple(map(float, t.bbox)), page_w=page_w, page_h=page_h)
                        except Exception:
                            bm = None

                        add_table(
                            tool="pdfplumber",
                            page=i + 1,
                            rows=rows,
                            meta={"settings": settings, "mode": mode},
                            bbox_meta=bm,
                        )

    except Exception as e:
        _eprint(f"WARNING: table extraction unavailable ({e}); skipping tables")

    return audit, out_md


def _docx_heading_level(style_name: str) -> Optional[int]:
    s = (style_name or "").strip()
    m = re.match(r"^Heading\s+(\d+)\b", s)
    if not m:
        return None
    try:
        lvl = int(m.group(1))
    except Exception:
        return None
    return max(1, min(6, lvl))


def _docx_clean_text(s: str) -> str:
    # Normalize whitespace; keep this deterministic.
    return re.sub(r"\s+", " ", (s or "").replace("\u00a0", " ").strip())


def extract_docx_markdown(docx_path: Path) -> str:
    try:
        from docx import Document  # type: ignore
    except Exception as e:
        raise RuntimeError("python-docx is required for DOCX parsing. Install it with: pip install python-docx") from e

    doc = Document(str(docx_path))
    out: List[str] = []

    for p in doc.paragraphs:
        txt = _docx_clean_text(p.text)
        if not txt:
            continue

        style_name = ""
        try:
            style_name = str(getattr(getattr(p, "style", None), "name", "") or "")
        except Exception:
            style_name = ""

        lvl = _docx_heading_level(style_name)
        if lvl is not None:
            out.append(f"{'#' * lvl} {txt}")
        else:
            out.append(txt)
        out.append("")

    return "\n".join(out).rstrip("\n") + "\n"


def extract_docx_text(docx_path: Path) -> str:
    # Plaintext companion used for explicit output paths (--raw-text-out).
    try:
        from docx import Document  # type: ignore
    except Exception as e:
        raise RuntimeError("python-docx is required for DOCX parsing. Install it with: pip install python-docx") from e

    doc = Document(str(docx_path))
    out: List[str] = []
    for p in doc.paragraphs:
        txt = _docx_clean_text(p.text)
        if txt:
            out.append(txt)
    return "\n".join(out).rstrip("\n") + "\n"


def _md_escape_cell(s: str) -> str:
    return (s or "").replace("|", "\\|")


def _rows_to_md_table(rows: List[List[str]]) -> List[str]:
    if not rows:
        return []
    max_cols = max((len(r) for r in rows), default=0)
    if max_cols <= 0:
        return []

    norm_rows: List[List[str]] = []
    for r in rows:
        rr = [
            _md_escape_cell(_docx_clean_text(c))
            for c in (r + [""] * (max_cols - len(r)))
        ]
        norm_rows.append(rr)

    header = norm_rows[0]
    lines = [
        "| " + " | ".join(header) + " |",
        "| " + " | ".join(["---"] * max_cols) + " |",
    ]
    for r in norm_rows[1:]:
        lines.append("| " + " | ".join(r) + " |")
    return lines


def extract_docx_tables(docx_path: Path) -> Tuple[List[Dict[str, Any]], List[str]]:
    try:
        from docx import Document  # type: ignore
    except Exception as e:
        raise RuntimeError("python-docx is required for DOCX parsing. Install it with: pip install python-docx") from e

    doc = Document(str(docx_path))
    audit: List[Dict[str, Any]] = []
    out_md: List[str] = []

    for i, t in enumerate(getattr(doc, "tables", []) or []):
        rows: List[List[str]] = []
        for row in getattr(t, "rows", []) or []:
            cells = []
            for cell in getattr(row, "cells", []) or []:
                cells.append(_docx_clean_text(getattr(cell, "text", "") or ""))
            rows.append(cells)

        md_lines = _rows_to_md_table(rows)
        if not md_lines:
            continue

        cols = max((len(r) for r in rows), default=0)
        audit.append({
            "tool": "python-docx",
            "table_index": int(i + 1),
            "rows": int(len(rows)),
            "cols": int(cols),
        })

        out_md.append(f"## Table {i + 1}")
        out_md.append("")
        out_md.extend(md_lines)
        out_md.append("")

    return audit, out_md


def main(argv: Optional[Sequence[str]] = None) -> int:
    ap = argparse.ArgumentParser(
        prog="tito-pdf",
        description="Convert a PDF/DOCX to Markdown (optionally tables).",
        epilog=f"Report bugs to: {__bugs_url__}. Home page: {__repo_url__}.",
    )
    ap.add_argument("input_path", help="Path to a PDF or DOCX file")

    ap.add_argument(
        "-V",
        "--version",
        action="version",
        version=f"tito-pdf {__version__}",
        help="output version information and exit",
    )

    ap.add_argument(
        "--mode",
        choices=["fast", "robust", "best"],
        default="robust",
        help=(
            "High-level mode (default: robust). "
            "fast disables OCR. best forces OCR and retries tables with lenient detection if strict finds none."
        ),
    )

    # Primary output (recommended)
    ap.add_argument("--md-out", default="", metavar="PATH", help="Write primary Markdown output to PATH")

    # Convenience output directory (only used when no explicit output paths are set)
    ap.add_argument(
        "--out-dir",
        default="",
        metavar="DIR",
        help="Convenience output directory (default: next to input). Ignored when any explicit output path is set.",
    )

    # Legacy/transitional: accept --id but do not promote it.
    ap.add_argument("--id", dest="legacy_id", default="", help=argparse.SUPPRESS)

    # Explicit outputs (paths)
    ap.add_argument("--raw-text-out", default="", metavar="PATH", help="Write extracted plaintext (UTF-8) to PATH")
    ap.add_argument("--tables-out", default="", metavar="PATH", help="Write extracted tables markdown to PATH")
    ap.add_argument(
        "--tables-audit-out",
        default="",
        metavar="PATH",
        help="Write tables audit JSON to PATH (requires --tables-out)",
    )
    ap.add_argument("--assets-json", default="", metavar="PATH", help="Write assets/metrics JSON to PATH")

    # Convenience toggles (only used when no explicit output paths are set)
    ap.add_argument("--text", action="store_true", help="Convenience mode: write Markdown output")
    ap.add_argument("--tables", action="store_true", help="Convenience mode: write tables markdown output")
    ap.add_argument("--all", action="store_true", help="Convenience mode: write both Markdown and tables outputs")

    ap.add_argument(
        "--tables-lenient",
        action="store_true",
        help="Enable text-based table detection (more tables, more false positives)",
    )

    ap.add_argument("--no-ocr", action="store_true", help="Disable OCR stage")
    ap.add_argument("--force-ocr", action="store_true", help="Force OCR even if PDF already has text")
    ap.add_argument("--max-pages", type=int, default=0, metavar="N", help="Limit pages processed (debug; 0 = all)")
    args = ap.parse_args(argv)

    legacy_id = str(getattr(args, "legacy_id", "") or "").strip()
    if legacy_id:
        _eprint(
            "WARNING: --id is deprecated and will be removed. "
            "tito-pdf is a standalone single-document tool; "
            "derive output names from the input filename or use --md-out for an explicit path."
        )

    input_path = Path(args.input_path).expanduser().resolve()
    if not input_path.is_file():
        _eprint(f"ERROR: input file not found: {input_path}")
        return 2

    suffix = input_path.suffix.lower()
    if suffix == ".pdf":
        input_kind = "pdf"
    elif suffix == ".docx":
        input_kind = "docx"
    else:
        _eprint(f"ERROR: unsupported input type (expected .pdf or .docx): {input_path}")
        return 2

    def _opt_path(v: str) -> Optional[Path]:
        vv = (v or "").strip()
        if not vv:
            return None
        return Path(vv).expanduser().resolve()

    def _opt_dir(v: str) -> Optional[Path]:
        vv = (v or "").strip()
        if not vv:
            return None
        return Path(vv).expanduser().resolve()

    md_out = _opt_path(getattr(args, "md_out", ""))
    raw_text_out = _opt_path(getattr(args, "raw_text_out", ""))
    tables_out = _opt_path(getattr(args, "tables_out", ""))
    tables_audit_out = _opt_path(getattr(args, "tables_audit_out", ""))
    assets_json_out = _opt_path(getattr(args, "assets_json", ""))

    explicit_output_mode = any([md_out, raw_text_out, tables_out, tables_audit_out, assets_json_out])

    # Determine what to generate + where to write it.
    do_md = False
    do_raw_text = False
    do_tables = False

    if explicit_output_mode:
        do_md = md_out is not None
        do_raw_text = raw_text_out is not None
        do_tables = tables_out is not None or tables_audit_out is not None

        if tables_audit_out is not None and tables_out is None:
            _eprint("ERROR: --tables-audit-out requires --tables-out")
            return 2

        if not do_md and not do_raw_text and not do_tables:
            _eprint("ERROR: no outputs requested. Use --md-out and/or --raw-text-out and/or --tables-out.")
            return 2

    else:
        # Convenience mode: write outputs next to input (or under --out-dir).
        out_dir = _opt_dir(getattr(args, "out_dir", ""))
        if out_dir is None:
            out_dir = input_path.parent
        else:
            out_dir.mkdir(parents=True, exist_ok=True)

        do_tables = bool(getattr(args, "tables", False)) or bool(getattr(args, "all", False))
        do_md = bool(getattr(args, "text", False)) or bool(getattr(args, "all", False))
        if not do_md and not do_tables:
            do_md = True  # default: markdown only

        if do_md:
            md_out = (out_dir / f"{input_path.stem}.md").resolve()
        if do_tables:
            tables_out = (out_dir / f"{input_path.stem}.tables.md").resolve()

    # Resolve high-level mode into effective knobs (explicit flags still win).
    mode = str(getattr(args, "mode", "robust") or "robust")
    explicit_no_ocr = bool(getattr(args, "no_ocr", False))
    explicit_force_ocr = bool(getattr(args, "force_ocr", False))
    explicit_tables_lenient = bool(getattr(args, "tables_lenient", False))

    no_ocr = bool(getattr(args, "no_ocr", False))
    force_ocr = bool(getattr(args, "force_ocr", False))
    tables_lenient = bool(getattr(args, "tables_lenient", False))
    tables_auto_fallback = False

    if mode == "fast":
        if not explicit_no_ocr and not explicit_force_ocr:
            no_ocr = True
    elif mode == "best":
        if not explicit_no_ocr and not explicit_force_ocr:
            force_ocr = True
        if not explicit_tables_lenient:
            tables_auto_fallback = True

    if no_ocr and force_ocr:
        _eprint("WARNING: both --no-ocr and --force-ocr were set; using --no-ocr")
        force_ocr = False

    max_pages = int(getattr(args, "max_pages", 0) or 0)

    started_at_utc = _utc_now_iso()
    t0 = time.monotonic()
    stage_timings_ms: Dict[str, int] = {}

    text_md_written: Optional[Path] = None
    raw_text_written: Optional[Path] = None
    tables_md_written: Optional[Path] = None
    tables_audit_written: Optional[Path] = None

    raw_text_bytes: int = 0
    raw_text_lines: int = 0
    tables_count: int = 0
    ocr_ran: Optional[bool] = None

    md_text: Optional[str] = None
    raw_text: Optional[str] = None
    tables_md_text: Optional[str] = None
    tables_audit_payload: Optional[Dict[str, Any]] = None

    toolchain: Optional[Dict[str, Any]] = None
    if assets_json_out is not None:
        qpdf_path = _which("qpdf")
        ocrmypdf_path = _which("ocrmypdf")
        tesseract_path = _which("tesseract")

        toolchain = {
            "python": {
                "executable": sys.executable,
                "version": sys.version.split()[0],
            },
            "platform": platform.platform(),
            "commands": {
                "qpdf": {"path": qpdf_path, "version": _cmd_version([qpdf_path, "--version"]) if qpdf_path else None},
                "ocrmypdf": {"path": ocrmypdf_path, "version": _cmd_version([ocrmypdf_path, "--version"]) if ocrmypdf_path else None},
                "tesseract": {"path": tesseract_path, "version": _cmd_version([tesseract_path, "--version"]) if tesseract_path else None},
            },
            "python_packages": {
                "PyMuPDF": _py_pkg_version("PyMuPDF"),
                "pdfplumber": _py_pkg_version("pdfplumber"),
                "pandas": _py_pkg_version("pandas"),
                "ocrmypdf": _py_pkg_version("ocrmypdf"),
                "python-docx": _py_pkg_version("python-docx"),
            },
        }

    def _write_text_atomic(path: Path, text: str) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        tmp = path.with_suffix(path.suffix + ".tmp")
        tmp.write_text(text, encoding="utf-8")
        tmp.replace(path)

    def _write_json_atomic(path: Path, obj: Any) -> None:
        _write_text_atomic(path, json.dumps(obj, indent=2, ensure_ascii=False) + "\n")

    with tempfile.TemporaryDirectory(prefix="tito-pdf-") as tmp_dir:
        work_dir = Path(tmp_dir)

        if input_kind == "pdf":
            prepared_pdf = work_dir / "prepared.pdf"
            t_stage = time.monotonic()
            try:
                prepare_pdf(input_pdf=input_path, output_pdf=prepared_pdf)
            except Exception as e:
                _eprint(f"ERROR: failed to prepare PDF: {e}")
                return 2
            stage_timings_ms["prepare_pdf_ms"] = int((time.monotonic() - t_stage) * 1000)

            ocr_out_pdf = work_dir / "ocr.pdf"
            if no_ocr:
                shutil.copy2(prepared_pdf, ocr_out_pdf)
                ocr_ran = False
            else:
                t_stage = time.monotonic()
                ocr_ran = ocr_pdf(input_pdf=prepared_pdf, output_pdf=ocr_out_pdf, force=bool(force_ocr))
                stage_timings_ms["ocr_ms"] = int((time.monotonic() - t_stage) * 1000)

            pdf_for_extract = ocr_out_pdf

            lines: Optional[List[PdfLine]] = None
            if do_md or do_raw_text:
                t_stage = time.monotonic()
                lines = extract_lines_layout(pdf_for_extract, max_pages=max_pages)
                lines = drop_repeated_headers_footers(lines)
                stage_timings_ms["extract_text_ms"] = int((time.monotonic() - t_stage) * 1000)

            if do_md and lines is not None:
                md_text = lines_to_markdown(lines)

            if do_raw_text and lines is not None:
                raw_text = lines_to_text(lines)
                raw_text_bytes = len(raw_text.encode("utf-8", errors="replace"))
                raw_text_lines = raw_text.count("\n")

            if do_tables:
                t_stage = time.monotonic()
                audit, table_md = extract_tables(
                    pdf_for_extract,
                    max_pages=max_pages,
                    lenient=bool(tables_lenient),
                )
                if not audit and tables_auto_fallback:
                    audit, table_md = extract_tables(
                        pdf_for_extract,
                        max_pages=max_pages,
                        lenient=True,
                    )

                stage_timings_ms["extract_tables_ms"] = int((time.monotonic() - t_stage) * 1000)
                tables_count = int(len(audit))

                if not table_md:
                    tables_md_text = "(No tables detected.)\n"
                else:
                    header = [
                        f"# Tables extracted from {input_path.name}",
                        "",
                        "This file is generated by tito-pdf (deterministic, no LLM).",
                        "",
                    ]
                    tables_md_text = "\n".join(header + table_md).rstrip("\n") + "\n"

                tables_audit_payload = {
                    "input_kind": "pdf",
                    "input_path": str(input_path),
                    "mode": str(mode),
                    "ocr_ran": bool(ocr_ran),
                    "tables": audit,
                }

        else:
            # input_kind == "docx"
            if do_md:
                t_stage = time.monotonic()
                md_text = extract_docx_markdown(input_path)
                stage_timings_ms["extract_text_ms"] = int((time.monotonic() - t_stage) * 1000)

            if do_raw_text:
                t_stage = time.monotonic()
                raw_text = extract_docx_text(input_path)
                stage_timings_ms["extract_text_ms"] = int((time.monotonic() - t_stage) * 1000)
                raw_text_bytes = len(raw_text.encode("utf-8", errors="replace"))
                raw_text_lines = raw_text.count("\n")

            if do_tables:
                t_stage = time.monotonic()
                audit, table_md = extract_docx_tables(input_path)
                stage_timings_ms["extract_tables_ms"] = int((time.monotonic() - t_stage) * 1000)
                tables_count = int(len(audit))

                if not table_md:
                    tables_md_text = "(No tables detected.)\n"
                else:
                    header = [
                        f"# Tables extracted from {input_path.name}",
                        "",
                        "This file is generated by tito-pdf (deterministic, no LLM).",
                        "",
                    ]
                    tables_md_text = "\n".join(header + table_md).rstrip("\n") + "\n"

                tables_audit_payload = {
                    "input_kind": "docx",
                    "input_path": str(input_path),
                    "mode": str(mode),
                    "tables": audit,
                }

    # Write requested outputs (after extraction succeeded)
    if md_out is not None:
        if md_text is None:
            _eprint("ERROR: failed to produce Markdown output")
            return 1
        _write_text_atomic(md_out, md_text)
        text_md_written = md_out
        _eprint(f"Wrote: {md_out}")

    if raw_text_out is not None:
        if raw_text is None:
            _eprint("ERROR: failed to produce raw text output")
            return 1
        _write_text_atomic(raw_text_out, raw_text)
        raw_text_written = raw_text_out
        _eprint(f"Wrote: {raw_text_out}")

    if tables_out is not None:
        if tables_md_text is None:
            _eprint("ERROR: failed to produce tables output")
            return 1
        _write_text_atomic(tables_out, tables_md_text)
        tables_md_written = tables_out
        _eprint(f"Wrote: {tables_out}")

    if tables_audit_out is not None:
        if tables_audit_payload is None:
            _eprint("ERROR: failed to produce tables audit output")
            return 1
        _write_json_atomic(tables_audit_out, tables_audit_payload)
        tables_audit_written = tables_audit_out
        _eprint(f"Wrote: {tables_audit_out}")

    if assets_json_out is not None:
        payload: Dict[str, Any] = {
            "schema_version": 1,
            "tool": "tito-pdf",
            "started_at_utc": started_at_utc,
            "finished_at_utc": _utc_now_iso(),
            "duration_ms": int((time.monotonic() - t0) * 1000),
            "mode": str(mode),
            "input_kind": str(input_kind),
            "input_path": str(input_path),
            "input_size_bytes": int(input_path.stat().st_size),
            "timings_ms": dict(stage_timings_ms),
            "metrics": {
                "raw_text_bytes": int(raw_text_bytes),
                "raw_text_lines": int(raw_text_lines),
                "tables_count": int(tables_count),
            },
        }

        outputs: Dict[str, Any] = {}
        if text_md_written is not None:
            outputs["text_md"] = str(text_md_written)
            payload["md_out"] = str(text_md_written)
        if raw_text_written is not None:
            outputs["raw_text"] = str(raw_text_written)
        if tables_md_written is not None:
            outputs["tables_md"] = str(tables_md_written)
        if tables_audit_written is not None:
            outputs["tables_audit_json"] = str(tables_audit_written)
        if outputs:
            payload["outputs"] = outputs

        # Preserve legacy top-level keys for integrations.
        if raw_text_out is not None:
            payload["raw_text_out"] = str(raw_text_out)
            payload["raw_text_bytes"] = int(raw_text_bytes)
            payload["raw_text_lines"] = int(raw_text_lines)
        if tables_out is not None:
            payload["tables_out"] = str(tables_out)
        if tables_audit_out is not None:
            payload["tables_audit_out"] = str(tables_audit_out)

        if toolchain is not None:
            payload["toolchain"] = toolchain

        _write_json_atomic(assets_json_out, payload)
        _eprint(f"Wrote: {assets_json_out}")

    _eprint("OK")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
